groups:
  - name: scraper-data-quality-critical
    interval: 30s
    rules:
      # ========================================================================
      # CRITICAL: Zero EnhancedTrackItem Creation
      # ========================================================================
      - alert: ScraperZeroEnhancedTracksCreated
        expr: |
          (
            increase(scraper_enhanced_tracks_total[6h]) == 0
          )
          AND
          (
            scraper_container_health_status == 1
          )
        for: 6h
        labels:
          severity: critical
          component: scraper
          team: data-platform
          priority: P1
        annotations:
          summary: "CRITICAL: {{ $labels.scraper_name }} has not created any EnhancedTrackItems in 6 hours"
          description: |
            Scraper {{ $labels.scraper_name }} has not created any EnhancedTrackItem objects
            in the last 6 hours despite being healthy. This indicates:
            - EnhancedTrackItem extraction has stopped
            - Pipeline is stuck or misconfigured
            - Schema mismatch preventing item creation

            Current status:
            - Enhanced tracks created (6h): {{ $value }}
            - Container health: healthy
            - Source: {{ $labels.source }}

            IMMEDIATE ACTIONS REQUIRED:
            1. Check scraper logs for extraction errors
            2. Verify EnhancedTrackItem schema matches database
            3. Check pipeline flush operations
            4. Verify Scrapy item loaders are functioning
          runbook_url: "https://docs.songnodes.com/runbooks/zero-enhanced-tracks"
          dashboard_url: "http://grafana:3000/d/scraper-data-quality"

      # ========================================================================
      # CRITICAL: High Schema Error Rate
      # ========================================================================
      - alert: ScraperHighSchemaErrorRate
        expr: |
          (
            rate(scraper_schema_errors_total[10m]) > 0.5
          )
          AND
          (
            rate(scraper_items_created_total[10m]) > 0
          )
        for: 5m
        labels:
          severity: critical
          component: scraper
          team: data-platform
          priority: P1
        annotations:
          summary: "CRITICAL: High schema error rate for {{ $labels.scraper_name }}"
          description: |
            Scraper {{ $labels.scraper_name }} is experiencing schema validation errors
            at a rate of {{ $value | humanize }} errors per second.

            Error details:
            - Error type: {{ $labels.error_type }}
            - Item type: {{ $labels.item_type }}
            - Rate: {{ $value | humanize }}/s

            This indicates:
            - Schema mismatch between scraper and database
            - Recent database migration not reflected in scraper
            - Data type incompatibilities

            ACTIONS REQUIRED:
            1. Check recent database schema changes
            2. Verify scraper item definitions match database
            3. Review validation pipeline logs
            4. Check Pydantic model definitions
          runbook_url: "https://docs.songnodes.com/runbooks/schema-errors"

      # ========================================================================
      # WARNING: Low EnhancedTrackItem Rate
      # ========================================================================
      - alert: ScraperLowEnhancedTrackRate
        expr: |
          (
            rate(scraper_enhanced_tracks_total[1h]) < 1.0
          )
          AND
          (
            rate(scraper_enhanced_tracks_total[1h]) > 0
          )
        for: 2h
        labels:
          severity: warning
          component: scraper
          team: data-platform
          priority: P2
        annotations:
          summary: "Low EnhancedTrackItem creation rate for {{ $labels.scraper_name }}"
          description: |
            Scraper {{ $labels.scraper_name }} is creating EnhancedTrackItems at
            a rate of only {{ $value | humanize }} per second (expected > 1/s).

            This may indicate:
            - Reduced data availability from source
            - Rate limiting or blocking
            - Performance degradation
            - Network issues

            Current metrics:
            - Rate: {{ $value | humanize }} items/s
            - ISRC availability: {{ $labels.has_isrc }}
            - Spotify ID availability: {{ $labels.has_spotify_id }}

            ACTIONS:
            1. Check source website availability
            2. Review rate limiting status
            3. Check network latency
            4. Review scraper performance metrics

  - name: scraper-data-quality-warnings
    interval: 60s
    rules:
      # ========================================================================
      # WARNING: Artist Coverage Drop
      # ========================================================================
      - alert: ScraperArtistCoverageDropped
        expr: |
          (
            (
              scraper_artist_coverage_percent -
              scraper_artist_coverage_percent offset 24h
            ) < -5
          )
          AND
          (
            scraper_artist_coverage_percent > 0
          )
        for: 1h
        labels:
          severity: warning
          component: scraper
          team: data-platform
          priority: P2
        annotations:
          summary: "Artist coverage dropped for {{ $labels.scraper_name }}"
          description: |
            Artist coverage for {{ $labels.scraper_name }} has dropped by more than 5%
            in the last 24 hours.

            Coverage change:
            - Current: {{ $value }}%
            - 24h ago: {{ $value | humanize }}%
            - Drop: {{ printf "%.2f" (sub $value (scraper_artist_coverage_percent offset 24h)) }}%

            Possible causes:
            - Artists removed from source
            - Scraping failures for specific artists
            - Target artist list changes

            ACTIONS:
            1. Review failed scraping attempts
            2. Check for artist-specific errors
            3. Verify target artist list
            4. Review source website changes

      # ========================================================================
      # WARNING: High Duplicate Rate
      # ========================================================================
      - alert: ScraperHighDuplicateRate
        expr: |
          (
            rate(scraper_duplicate_items_total[1h]) /
            (rate(scraper_items_created_total[1h]) + 0.001)
          ) > 0.3
        for: 30m
        labels:
          severity: warning
          component: scraper
          team: data-platform
          priority: P3
        annotations:
          summary: "High duplicate detection rate for {{ $labels.scraper_name }}"
          description: |
            {{ $labels.scraper_name }} is detecting {{ $value | humanizePercentage }}
            duplicates among created items.

            Duplicate metrics:
            - Item type: {{ $labels.item_type }}
            - Dedup strategy: {{ $labels.dedup_strategy }}
            - Rate: {{ $value | humanizePercentage }}

            This may indicate:
            - Inefficient scraping (revisiting same sources)
            - Deduplication strategy issues
            - Increased data overlap between sources

            ACTIONS:
            1. Review scraping strategy
            2. Check deduplication logic
            3. Analyze source overlap
            4. Consider adjusting scraping schedule

      # ========================================================================
      # WARNING: Low Playlist Discovery Rate
      # ========================================================================
      - alert: ScraperLowPlaylistDiscoveryRate
        expr: |
          rate(scraper_playlists_discovered_total[2h]) < 0.1
        for: 2h
        labels:
          severity: warning
          component: scraper
          team: data-platform
          priority: P3
        annotations:
          summary: "Low playlist discovery rate for {{ $labels.scraper_name }}"
          description: |
            {{ $labels.scraper_name }} is discovering playlists/tracklists at
            a rate of {{ $value | humanize }} per second (expected > 0.1/s).

            Current metrics:
            - Discovery rate: {{ $value | humanize }} playlists/s
            - Source: {{ $labels.source }}

            Possible causes:
            - Source has fewer playlists available
            - Search queries not returning results
            - Website structure changes
            - Rate limiting

            ACTIONS:
            1. Verify source availability
            2. Check search query effectiveness
            3. Review website structure
            4. Monitor rate limiting status

  - name: scraper-enrichment-quality
    interval: 60s
    rules:
      # ========================================================================
      # WARNING: High Enrichment Failure Rate
      # ========================================================================
      - alert: ScraperHighEnrichmentFailureRate
        expr: |
          (
            rate(scraper_enrichment_failures_total[15m]) /
            (
              rate(scraper_enrichment_success_total[15m]) +
              rate(scraper_enrichment_failures_total[15m]) +
              0.001
            )
          ) > 0.2
        for: 10m
        labels:
          severity: warning
          component: enrichment
          team: data-platform
          priority: P2
        annotations:
          summary: "High enrichment failure rate for {{ $labels.scraper_name }}"
          description: |
            Data enrichment for {{ $labels.scraper_name }} is failing at
            {{ $value | humanizePercentage }} rate.

            Failure details:
            - Enrichment source: {{ $labels.enrichment_source }}
            - Failure reason: {{ $labels.failure_reason }}
            - Rate: {{ $value | humanizePercentage }}

            Common causes:
            - API rate limiting
            - Invalid API credentials
            - Network connectivity issues
            - External service outages

            ACTIONS:
            1. Check external API status
            2. Verify API credentials
            3. Review rate limiting
            4. Check circuit breaker status

      # ========================================================================
      # INFO: Low NLP Extraction Confidence
      # ========================================================================
      - alert: ScraperLowNLPConfidence
        expr: |
          (
            histogram_quantile(0.5,
              rate(scraper_nlp_extraction_confidence_bucket[30m])
            ) < 0.7
          )
        for: 30m
        labels:
          severity: info
          component: nlp
          team: data-platform
          priority: P4
        annotations:
          summary: "Low NLP extraction confidence for {{ $labels.scraper_name }}"
          description: |
            NLP extraction confidence for {{ $labels.scraper_name }} is low.
            Median confidence: {{ $value | humanizePercentage }}

            Extraction type: {{ $labels.extraction_type }}

            This may indicate:
            - Unstructured or inconsistent data
            - NLP model needs retraining
            - Source format changes

            ACTIONS:
            1. Review extraction samples
            2. Check source data quality
            3. Consider model retraining
            4. Verify extraction patterns

  - name: scraper-validation-quality
    interval: 60s
    rules:
      # ========================================================================
      # CRITICAL: Low Validation Success Rate
      # ========================================================================
      - alert: ScraperLowValidationSuccessRate
        expr: |
          scraper_validation_success_rate < 0.8
        for: 15m
        labels:
          severity: critical
          component: validation
          team: data-platform
          priority: P1
        annotations:
          summary: "Low validation success rate for {{ $labels.scraper_name }}"
          description: |
            Validation success rate for {{ $labels.scraper_name }} has dropped
            to {{ $value | humanizePercentage }}.

            Validation type: {{ $labels.validation_type }}

            This indicates:
            - Data quality issues
            - Schema mismatches
            - Invalid data from source

            ACTIONS REQUIRED:
            1. Review validation failures
            2. Check data quality at source
            3. Verify validation rules
            4. Update data cleaning logic

      # ========================================================================
      # WARNING: High Validation Error Rate
      # ========================================================================
      - alert: ScraperHighValidationErrorRate
        expr: |
          rate(scraper_validation_errors_total[10m]) > 1.0
        for: 10m
        labels:
          severity: warning
          component: validation
          team: data-platform
          priority: P2
        annotations:
          summary: "High validation error rate for {{ $labels.scraper_name }}"
          description: |
            Validation errors for {{ $labels.scraper_name }} occurring at
            {{ $value | humanize }} errors per second.

            Error details:
            - Stage: {{ $labels.stage }}
            - Error type: {{ $labels.error_type }}
            - Rate: {{ $value | humanize }}/s

            ACTIONS:
            1. Review validation pipeline logs
            2. Check data quality
            3. Verify validation rules
            4. Update error handling
