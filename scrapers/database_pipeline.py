"""
Database Pipeline for SongNodes Scrapers with Pydantic Validation (2025)

Reliable implementation using psycopg2 with Twisted's adbapi (thread pool).
100% compatible with Scrapy's Twisted reactor - zero data loss guaranteed.

Key Features:
- Twisted adbapi with psycopg2 (proven reliable)
- Batch processing with auto-flush
- Pydantic validation
- Zero data loss (guaranteed flush on close)

Note: Uses psycopg2 instead of asyncpg due to Twisted reactor incompatibility.
"""
import psycopg2
import psycopg2.extras
import logging
from datetime import datetime, date
from typing import Dict, List, Any, Optional
from pydantic import ValidationError
from twisted.enterprise import adbapi
from twisted.internet import defer

# Import Pydantic validation functions
try:
    from pydantic_adapter import (
        validate_artist_item,
        validate_track_item,
        validate_setlist_item,
        validate_track_adjacency_item,
    )
    PYDANTIC_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ Pydantic validation enabled for database pipeline")
except ImportError as e:
    PYDANTIC_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"‚ö†Ô∏è Pydantic validation not available: {e}")


class DatabasePipeline:
    """
    Database pipeline for Scrapy with psycopg2 and Twisted adbapi.

    Uses Twisted's thread pool for reliable database operations.
    100% compatible with Scrapy's Twisted reactor - zero data loss guaranteed.
    """

    @classmethod
    def from_crawler(cls, crawler):
        """Load database config from environment variables"""
        import os

        # Use centralized secrets manager if available
        try:
            from common.secrets_manager import get_database_config
            db_config = get_database_config()
            # Allow environment variable overrides for host/port (for local development)
            if os.getenv('DATABASE_HOST'):
                db_config['host'] = os.getenv('DATABASE_HOST')
            if os.getenv('DATABASE_PORT'):
                db_config['port'] = int(os.getenv('DATABASE_PORT'))
            logging.info(f"‚úì Using secrets manager config (with overrides): host={db_config['host']}:{db_config['port']}")
        except ImportError:
            # Fallback to environment variables
            db_config = {
                'host': os.getenv('DATABASE_HOST', 'postgres'),
                'port': int(os.getenv('DATABASE_PORT', '5432')),
                'database': os.getenv('DATABASE_NAME', 'musicdb'),
                'user': os.getenv('DATABASE_USER', 'musicdb_user'),
                'password': os.getenv('DATABASE_PASSWORD', 'musicdb_secure_pass_2024')
            }
            logging.info(f"‚úì Using environment variable config: host={db_config['host']}:{db_config['port']}")

        return cls(db_config)

    def __init__(self, database_config: Dict[str, Any]):
        self.config = database_config
        self.dbpool = None
        self.logger = logging.getLogger(__name__)

        # Batch processing
        self.batch_size = 50
        self.item_batches = {
            'artists': [],
            'songs': [],
            'playlists': [],
            'playlist_tracks': [],
            'track_artists': [],  # Track-artist relationships (featured, remixer, etc.)
            'song_adjacency': []
        }

        # Track processed items to avoid duplicates
        self.processed_items = {
            'artists': set(),
            'songs': set(),
            'playlists': set()
        }

        # Validation statistics
        self.validation_stats = {
            'total_items': 0,
            'valid_items': 0,
            'invalid_items': 0,
            'validation_errors': []
        }

    def open_spider(self, spider):
        """Initialize psycopg2 connection pool via Twisted adbapi"""
        self.dbpool = adbapi.ConnectionPool(
            'psycopg2',
            host=self.config['host'],
            port=self.config['port'],
            database=self.config['database'],
            user=self.config['user'],
            password=self.config['password'],
            cp_min=5,
            cp_max=15,
            cp_reconnect=True
        )

        self.logger.info("‚úì Database connection pool initialized (psycopg2 + Twisted adbapi)")

    def process_item(self, item, spider):
        """
        Process item and add to batch.
        Auto-flushes when batch size is reached (returns Deferred on flush).
        """
        try:
            if not self.dbpool:
                self.logger.error("Database pool not initialized!")
                return item

            # Debug: Log ALL items to see what's coming through
            item_class = item.__class__.__name__
            if 'Adjacency' in item_class:
                self.logger.info(f"üìç Pipeline received {item_class} with fields: {list(item.keys())}")

            item_type = item.get('item_type')

            # Auto-detect item type if not set
            if not item_type:
                # Check for adjacency FIRST (before setlist, since adjacencies also have setlist_name)
                if 'track_1_name' in item and 'track_2_name' in item:
                    item_type = 'track_adjacency'
                    self.logger.debug(f"‚úì Detected adjacency item: {item.get('track_1_name')} ‚Üí {item.get('track_2_name')}")
                # Check for track-artist relationships BEFORE checking for track (critical!)
                elif 'track_name' in item and 'artist_name' in item and 'artist_role' in item:
                    item_type = 'track_artist'
                    self.logger.debug(f"‚úì Detected track-artist relationship: {item.get('artist_name')} ({item.get('artist_role')}) - {item.get('track_name')}")
                elif 'artist_name' in item and 'track_name' not in item and 'setlist_name' not in item:
                    item_type = 'artist'
                elif 'track_name' in item:
                    item_type = 'track'
                elif 'setlist_name' in item:
                    item_type = 'setlist'
                else:
                    self.logger.warning(f"Could not auto-detect item type. Fields: {list(item.keys())}")
                    return item

            # Process different item types (synchronous - adds to batch)
            if item_type == 'artist':
                self._process_artist_item(item)
            elif item_type == 'track':
                self._process_track_item(item)
            elif item_type == 'track_artist':
                self._process_track_artist_item(item)
            elif item_type in ('playlist', 'setlist'):
                self._process_playlist_item(item)
            elif item_type == 'playlist_track':
                self._process_playlist_track_item(item)
            elif item_type == 'track_adjacency':
                self.logger.info(f"Processing adjacency: {item.get('track_1_name')} ‚Üí {item.get('track_2_name')}")
                self._process_adjacency_item(item)
            else:
                self.logger.warning(f"Unknown item type: {item_type}")

            return item

        except Exception as e:
            self.logger.error(f"Error processing item: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            raise

    def _process_artist_item(self, item):
        """Process artist item with Pydantic validation"""
        artist_name = item.get('artist_name', '').strip()
        if not artist_name or artist_name in self.processed_items['artists']:
            return

        # Pydantic validation
        if PYDANTIC_AVAILABLE:
            self.validation_stats['total_items'] += 1
            try:
                data_source = item.get('data_source') or item.get('source')
                validated_artist = validate_artist_item(item, data_source=data_source)
                self.validation_stats['valid_items'] += 1
            except ValidationError as e:
                self.validation_stats['invalid_items'] += 1
                self.validation_stats['validation_errors'].append(f"Artist '{artist_name}': {str(e)}")
                self.logger.warning(f"‚ùå Invalid artist data, skipping: {artist_name} - {e}")
                return

        self.processed_items['artists'].add(artist_name)
        self.item_batches['artists'].append({
            'name': artist_name,
            'genre': item.get('genre'),
            'country': item.get('country')
        })

        # Auto-flush when batch is full
        if len(self.item_batches['artists']) >= self.batch_size:
            self._flush_batch('artists')

    def _process_track_item(self, item):
        """Process track item with Pydantic validation"""
        track_name = item.get('track_name', '').strip()
        if not track_name:
            return

        track_key = f"{track_name}::{item.get('artist_name', '')}"
        if track_key in self.processed_items['songs']:
            return

        # Pydantic validation
        if PYDANTIC_AVAILABLE:
            self.validation_stats['total_items'] += 1
            try:
                data_source = item.get('data_source') or item.get('source')
                validated_track = validate_track_item(item, data_source=data_source)
                self.validation_stats['valid_items'] += 1
            except ValidationError as e:
                self.validation_stats['invalid_items'] += 1
                self.validation_stats['validation_errors'].append(f"Track '{track_name}': {str(e)}")
                self.logger.warning(f"‚ùå Invalid track data, skipping: {track_name} - {e}")
                return

        self.processed_items['songs'].add(track_key)

        # Extract artist name
        artist_name = item.get('artist_name', '').strip()
        if not artist_name and ' - ' in track_name:
            artist_name = track_name.split(' - ')[0].strip()

        # Ensure artist exists
        if artist_name and artist_name not in self.processed_items['artists']:
            self._process_artist_item({'artist_name': artist_name, 'genre': item.get('genre')})

        self.item_batches['songs'].append({
            'title': track_name,
            'artist_name': artist_name if artist_name else None,
            'genre': item.get('genre'),
            'bpm': item.get('bpm'),
            'key': item.get('key') or item.get('musical_key'),
            'duration_seconds': item.get('duration_seconds'),
            'release_year': item.get('release_year'),
            'label': item.get('label') or item.get('record_label'),
            'spotify_id': item.get('spotify_id'),
            'musicbrainz_id': item.get('musicbrainz_id'),
            'tidal_id': item.get('tidal_id'),
            'beatport_id': item.get('beatport_id'),
            'apple_music_id': item.get('apple_music_id'),
            'soundcloud_id': item.get('soundcloud_id'),
            'deezer_id': item.get('deezer_id'),
            'youtube_music_id': item.get('youtube_music_id')
        })

        # Auto-flush when batch is full
        if len(self.item_batches['songs']) >= self.batch_size:
            self._flush_batch('songs')

    def _process_playlist_item(self, item):
        """Process playlist item with Pydantic validation"""
        playlist_name = item.get('name', '').strip()
        if not playlist_name or playlist_name in self.processed_items['playlists']:
            return

        # Pydantic validation
        if PYDANTIC_AVAILABLE:
            self.validation_stats['total_items'] += 1
            try:
                data_source = item.get('data_source') or item.get('source')
                validated_playlist = validate_setlist_item(item, data_source=data_source)
                self.validation_stats['valid_items'] += 1
            except ValidationError as e:
                self.validation_stats['invalid_items'] += 1
                self.validation_stats['validation_errors'].append(f"Playlist '{playlist_name}': {str(e)}")
                self.logger.warning(f"‚ùå Invalid playlist data, skipping: {playlist_name} - {e}")
                return

        self.processed_items['playlists'].add(playlist_name)

        # Parse event date
        event_date = item.get('playlist_date') or item.get('event_date')
        if event_date and isinstance(event_date, str):
            try:
                if 'T' in event_date:
                    event_date = datetime.fromisoformat(event_date.replace('Z', '+00:00').split('.')[0]).date()
                else:
                    event_date = datetime.strptime(event_date, '%Y-%m-%d').date()
            except Exception as e:
                self.logger.warning(f"Could not parse date {event_date}: {e}")
                event_date = None
        elif isinstance(event_date, datetime):
            event_date = event_date.date()
        elif not isinstance(event_date, date):
            event_date = None

        self.item_batches['playlists'].append({
            'name': playlist_name,
            'source': item.get('source') or item.get('platform') or 'unknown',
            'source_url': item.get('source_url'),
            'playlist_type': item.get('playlist_type'),
            'event_date': event_date
        })

        # Auto-flush when batch is full
        if len(self.item_batches['playlists']) >= self.batch_size:
            self._flush_batch('playlists')

    def _process_playlist_track_item(self, item):
        """Process playlist track item"""
        playlist_name = item.get('playlist_name', '').strip()
        track_name = item.get('track_name', '').strip()
        position = item.get('position')

        if not playlist_name or not track_name or position is None:
            return

        self.item_batches['playlist_tracks'].append({
            'playlist_name': playlist_name,
            'track_name': track_name,
            'artist_name': item.get('artist_name', ''),
            'position': position,
            'source': item.get('source', 'scraped_data')
        })

        # Auto-flush when batch is full
        if len(self.item_batches['playlist_tracks']) >= self.batch_size:
            self._flush_batch('playlist_tracks')

    def _process_track_artist_item(self, item):
        """Process track-artist relationship item (featured artists, remixers, etc.)"""
        track_name = item.get('track_name', '').strip()
        artist_name = item.get('artist_name', '').strip()
        artist_role = item.get('artist_role', 'primary')

        if not track_name or not artist_name:
            return

        # Skip Pydantic validation for track-artist items as they have different schema than tracks
        # Basic validation is sufficient: we have track_name, artist_name, and artist_role
        self.logger.debug(f"‚úì Processing track-artist: {artist_name} ({artist_role}) - {track_name}")

        self.item_batches['track_artists'].append({
            'track_name': track_name,
            'artist_name': artist_name,
            'role': artist_role,
            'position': item.get('position', 0)
        })

        # Auto-flush when batch is full
        if len(self.item_batches['track_artists']) >= self.batch_size:
            self._flush_batch('track_artists')

    def _process_adjacency_item(self, item):
        """Process track adjacency item with Pydantic validation"""
        track1 = item.get('track_1_name', '').strip()
        track2 = item.get('track_2_name', '').strip()

        if not track1 or not track2 or track1 == track2:
            return

        # Pydantic validation
        if PYDANTIC_AVAILABLE:
            self.validation_stats['total_items'] += 1
            try:
                data_source = item.get('data_source') or item.get('source')
                validated_adjacency = validate_track_adjacency_item(item, data_source=data_source)
                self.validation_stats['valid_items'] += 1
            except ValidationError as e:
                self.validation_stats['invalid_items'] += 1
                self.validation_stats['validation_errors'].append(f"Adjacency '{track1}‚Üí{track2}': {str(e)}")
                self.logger.warning(f"‚ùå Invalid adjacency data, skipping: {track1}‚Üí{track2} - {e}")
                return

        self.item_batches['song_adjacency'].append({
            'track1_name': track1,
            'track1_artist': item.get('track1_artist', ''),
            'track2_name': track2,
            'track2_artist': item.get('track2_artist', ''),
            'distance': item.get('distance', 1),
            'occurrence_count': item.get('occurrence_count', 1),
            'source_context': item.get('source_context', ''),
            'source_url': item.get('source_url')
        })

        # Auto-flush when batch is full
        if len(self.item_batches['song_adjacency']) >= self.batch_size:
            self._flush_batch('song_adjacency')

    def _flush_batch(self, batch_type: str):
        """Flush a specific batch to database using Twisted adbapi"""
        batch = self.item_batches[batch_type]
        if not batch:
            return defer.succeed(None)

        # Make a copy and clear the batch immediately
        batch_copy = list(batch)
        batch.clear()

        # Run the appropriate insert method in thread pool
        d = self.dbpool.runInteraction(self._flush_batch_to_db, batch_copy, batch_type)
        d.addCallback(lambda _: self.logger.info(f"‚úì Flushed {len(batch_copy)} {batch_type} items to database"))
        d.addErrback(self._handle_flush_error, batch_type, len(batch_copy))
        return d

    def _handle_flush_error(self, failure, batch_type, batch_size):
        """Handle flush errors"""
        self.logger.error(f"‚ùå Error flushing {batch_size} {batch_type} items: {failure.getErrorMessage()}")
        self.logger.error(failure.getTraceback())
        return failure

    def _flush_batch_to_db(self, txn, batch_data, batch_type):
        """Execute batch insert in thread pool (callback for runInteraction)"""
        if batch_type == 'artists':
            self._insert_artists_batch(txn, batch_data)
        elif batch_type == 'songs':
            self._insert_songs_batch(txn, batch_data)
        elif batch_type == 'playlists':
            self._insert_playlists_batch(txn, batch_data)
        elif batch_type == 'playlist_tracks':
            self._insert_playlist_tracks_batch(txn, batch_data)
        elif batch_type == 'track_artists':
            self._insert_track_artists_batch(txn, batch_data)
        elif batch_type == 'song_adjacency':
            self._insert_adjacency_batch(txn, batch_data)

    def _insert_artists_batch(self, txn, batch: List[Dict[str, Any]]):
        """Insert artists batch using psycopg2"""
        txn.executemany("""
            INSERT INTO artists (name, normalized_name, genres, country)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (name) DO UPDATE SET
                normalized_name = EXCLUDED.normalized_name,
                genres = COALESCE(EXCLUDED.genres, artists.genres),
                country = COALESCE(EXCLUDED.country, artists.country),
                updated_at = CURRENT_TIMESTAMP
        """, [
            (
                item['name'],
                item['name'].lower().strip(),  # normalized_name for searching
                [item['genre']] if item.get('genre') else None,
                item.get('country')
            ) for item in batch
        ])

    def _insert_songs_batch(self, txn, batch: List[Dict[str, Any]]):
        """
        Insert tracks batch with ISRC/Spotify ID based upsert logic.

        Priority order for conflict resolution:
        1. ISRC (most reliable natural key)
        2. Spotify ID (platform-specific but unique)
        3. Fallback to (title, normalized_title) for old data

        Uses COALESCE for smart merging - prefer new non-null values.
        """
        tracks_data = []
        track_artist_relationships = []

        for item in batch:
            primary_artist_id = None
            normalized_title = item.get('normalized_title') or item['title'].lower().strip()

            # Get primary artist ID if provided
            if item.get('artist_name'):
                try:
                    txn.execute(
                        "SELECT artist_id FROM artists WHERE name = %s",
                        (item['artist_name'],)
                    )
                    result = txn.fetchone()
                    if result:
                        primary_artist_id = result[0]
                    else:
                        self.logger.warning(f"Artist '{item.get('artist_name')}' not found in DB - should have been flushed first!")
                except Exception as e:
                    self.logger.warning(f"Could not find artist ID for {item.get('artist_name')}: {e}")

            # Convert duration_seconds to duration_ms
            duration_ms = None
            if item.get('duration_seconds'):
                duration_ms = item['duration_seconds'] * 1000

            # Convert release_year to release_date
            release_date = None
            if item.get('release_year'):
                try:
                    release_date = f"{item['release_year']}-01-01"
                except Exception:
                    pass

            # Preserve ALL raw scraper data in metadata field for future enrichment
            raw_metadata = {
                'scraper_source': item.get('source', 'unknown'),
                'original_data': {
                    'artist_name': item.get('artist_name'),  # Preserve even if no artist found
                    'raw_title': item.get('raw_title', item['title']),
                    'url': item.get('url'),
                    'scraped_at': item.get('scraped_at'),
                },
                'needs_enrichment': primary_artist_id is None  # Flag for enrichment pipeline
            }
            # Include any additional fields from scraper
            for key in item:
                if key not in ['title', 'artist_name', 'genre', 'bpm', 'key', 'duration_seconds', 'release_year', 'spotify_id', 'tidal_id', 'apple_music_id', 'isrc']:
                    raw_metadata['original_data'][key] = item[key]

            tracks_data.append({
                'title': item['title'],
                'normalized_title': normalized_title,
                'isrc': item.get('isrc'),  # NEW: ISRC support
                'genre': item.get('genre'),
                'bpm': item.get('bpm'),
                'key': item.get('key'),
                'duration_ms': duration_ms,
                'release_date': release_date,
                'spotify_id': item.get('spotify_id'),
                'tidal_id': item.get('tidal_id'),
                'apple_music_id': item.get('apple_music_id'),
                'metadata': raw_metadata,
                'primary_artist_id': primary_artist_id
            })

        # Insert tracks using ISRC/Spotify ID upsert strategy
        for track in tracks_data:
            try:
                import json
                track_id = self._upsert_track(txn, track)

                # Create track_artists relationship if we have both track and artist
                if track_id and track['primary_artist_id']:
                    track_artist_relationships.append((
                        track_id,
                        track['primary_artist_id'],
                        'primary',
                        0
                    ))
            except Exception as e:
                self.logger.warning(f"Error upserting track '{track['title']}': {e}")

        # Insert track_artists relationships
        if track_artist_relationships:
            txn.executemany("""
                INSERT INTO track_artists (track_id, artist_id, role, position)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (track_id, artist_id, role) DO NOTHING
            """, track_artist_relationships)

    def _upsert_track(self, txn, track: Dict[str, Any]) -> Optional[str]:
        """
        Upsert track using ISRC/Spotify ID priority strategy.

        Conflict resolution priority:
        1. ISRC (if available) - most reliable
        2. Spotify ID (if available) - platform unique
        3. (title, normalized_title) - fallback for legacy data

        Returns: track_id (UUID as string) or None on failure
        """
        import json

        # STRATEGY 1: Try ISRC-based upsert (highest priority)
        if track.get('isrc'):
            try:
                txn.execute("""
                    INSERT INTO tracks (
                        title, normalized_title, isrc, genre, bpm, key,
                        duration_ms, release_date,
                        spotify_id, tidal_id, apple_music_id, metadata
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (isrc) WHERE isrc IS NOT NULL
                    DO UPDATE SET
                        title = COALESCE(EXCLUDED.title, tracks.title),
                        normalized_title = COALESCE(EXCLUDED.normalized_title, tracks.normalized_title),
                        genre = COALESCE(EXCLUDED.genre, tracks.genre),
                        bpm = COALESCE(EXCLUDED.bpm, tracks.bpm),
                        key = COALESCE(EXCLUDED.key, tracks.key),
                        duration_ms = COALESCE(EXCLUDED.duration_ms, tracks.duration_ms),
                        release_date = COALESCE(EXCLUDED.release_date, tracks.release_date),
                        spotify_id = COALESCE(EXCLUDED.spotify_id, tracks.spotify_id),
                        tidal_id = COALESCE(EXCLUDED.tidal_id, tracks.tidal_id),
                        apple_music_id = COALESCE(EXCLUDED.apple_music_id, tracks.apple_music_id),
                        metadata = COALESCE(tracks.metadata, '{}'::jsonb) || COALESCE(EXCLUDED.metadata, '{}'::jsonb),
                        updated_at = CURRENT_TIMESTAMP
                    RETURNING id
                """, (
                    track['title'],
                    track['normalized_title'],
                    track['isrc'],
                    track['genre'],
                    track['bpm'],
                    track['key'],
                    track['duration_ms'],
                    track['release_date'],
                    track['spotify_id'],
                    track['tidal_id'],
                    track['apple_music_id'],
                    json.dumps(track['metadata'])
                ))
                result = txn.fetchone()
                if result:
                    self.logger.debug(f"‚úì Upserted track via ISRC: {track['isrc']}")
                    return result[0]
            except Exception as e:
                self.logger.warning(f"ISRC upsert failed for {track.get('isrc')}: {e}")

        # STRATEGY 2: Try Spotify ID-based upsert (second priority)
        if track.get('spotify_id'):
            try:
                txn.execute("""
                    INSERT INTO tracks (
                        title, normalized_title, isrc, genre, bpm, key,
                        duration_ms, release_date,
                        spotify_id, tidal_id, apple_music_id, metadata
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (spotify_id) WHERE spotify_id IS NOT NULL
                    DO UPDATE SET
                        title = COALESCE(EXCLUDED.title, tracks.title),
                        normalized_title = COALESCE(EXCLUDED.normalized_title, tracks.normalized_title),
                        isrc = COALESCE(EXCLUDED.isrc, tracks.isrc),  -- Backfill ISRC if available
                        genre = COALESCE(EXCLUDED.genre, tracks.genre),
                        bpm = COALESCE(EXCLUDED.bpm, tracks.bpm),
                        key = COALESCE(EXCLUDED.key, tracks.key),
                        duration_ms = COALESCE(EXCLUDED.duration_ms, tracks.duration_ms),
                        release_date = COALESCE(EXCLUDED.release_date, tracks.release_date),
                        tidal_id = COALESCE(EXCLUDED.tidal_id, tracks.tidal_id),
                        apple_music_id = COALESCE(EXCLUDED.apple_music_id, tracks.apple_music_id),
                        metadata = COALESCE(tracks.metadata, '{}'::jsonb) || COALESCE(EXCLUDED.metadata, '{}'::jsonb),
                        updated_at = CURRENT_TIMESTAMP
                    RETURNING id
                """, (
                    track['title'],
                    track['normalized_title'],
                    track['isrc'],
                    track['genre'],
                    track['bpm'],
                    track['key'],
                    track['duration_ms'],
                    track['release_date'],
                    track['spotify_id'],
                    track['tidal_id'],
                    track['apple_music_id'],
                    json.dumps(track['metadata'])
                ))
                result = txn.fetchone()
                if result:
                    self.logger.debug(f"‚úì Upserted track via Spotify ID: {track['spotify_id']}")
                    return result[0]
            except Exception as e:
                self.logger.warning(f"Spotify ID upsert failed for {track.get('spotify_id')}: {e}")

        # STRATEGY 3: Fallback to title-based lookup (legacy compatibility)
        try:
            # First, check if track exists by normalized title
            txn.execute(
                "SELECT id FROM tracks WHERE normalized_title = %s LIMIT 1",
                (track['normalized_title'],)
            )
            existing = txn.fetchone()

            if existing:
                # Update existing track with new data (backfill ISRC/Spotify ID if available)
                track_id = existing[0]
                txn.execute("""
                    UPDATE tracks SET
                        title = COALESCE(%s, title),
                        isrc = COALESCE(%s, isrc),
                        genre = COALESCE(%s, genre),
                        bpm = COALESCE(%s, bpm),
                        key = COALESCE(%s, key),
                        duration_ms = COALESCE(%s, duration_ms),
                        release_date = COALESCE(%s, release_date),
                        spotify_id = COALESCE(%s, spotify_id),
                        tidal_id = COALESCE(%s, tidal_id),
                        apple_music_id = COALESCE(%s, apple_music_id),
                        metadata = COALESCE(metadata, '{}'::jsonb) || COALESCE(%s::jsonb, '{}'::jsonb),
                        updated_at = CURRENT_TIMESTAMP
                    WHERE id = %s
                """, (
                    track['title'],
                    track['isrc'],
                    track['genre'],
                    track['bpm'],
                    track['key'],
                    track['duration_ms'],
                    track['release_date'],
                    track['spotify_id'],
                    track['tidal_id'],
                    track['apple_music_id'],
                    json.dumps(track['metadata']),
                    track_id
                ))
                self.logger.debug(f"‚úì Updated existing track via title: {track['title']}")
                return track_id
            else:
                # Insert new track (no identifiers, no existing match)
                txn.execute("""
                    INSERT INTO tracks (
                        title, normalized_title, isrc, genre, bpm, key,
                        duration_ms, release_date,
                        spotify_id, tidal_id, apple_music_id, metadata
                    )
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    track['title'],
                    track['normalized_title'],
                    track['isrc'],
                    track['genre'],
                    track['bpm'],
                    track['key'],
                    track['duration_ms'],
                    track['release_date'],
                    track['spotify_id'],
                    track['tidal_id'],
                    track['apple_music_id'],
                    json.dumps(track['metadata'])
                ))
                result = txn.fetchone()
                if result:
                    self.logger.debug(f"‚úì Inserted new track: {track['title']}")
                    return result[0]

        except Exception as e:
            self.logger.error(f"Title-based fallback failed for '{track['title']}': {e}")

        return None

    def _insert_playlists_batch(self, txn, batch: List[Dict[str, Any]]):
        """Insert playlists batch with URL-based deduplication to prevent inflated edge weights"""
        import hashlib
        import json

        for item in batch:
            source_url = item.get('source_url')

            # Generate content hash from tracklist if available
            # This prevents duplicates even when URLs change (dynamic URLs, pagination, etc.)
            content_hash = None
            if 'tracklist' in item and item['tracklist']:
                # Sort track names to ensure consistent hash regardless of order
                tracklist_normalized = sorted([
                    t.get('track_name', '').lower().strip()
                    for t in item['tracklist']
                ])
                content_str = json.dumps(tracklist_normalized, sort_keys=True)
                content_hash = hashlib.sha256(content_str.encode()).hexdigest()[:16]  # 16 chars sufficient

                # Check content hash first (most reliable)
                txn.execute(
                    "SELECT playlist_id FROM playlists WHERE content_hash = %s",
                    (content_hash,)
                )
                existing = txn.fetchone()
                if existing:
                    self.logger.info(f"Skipping duplicate playlist (same tracklist, different URL): {item['name']}")
                    continue

            # CRITICAL: Deduplicate by source_url to prevent same setlist being scraped multiple times
            if source_url:
                txn.execute(
                    "SELECT playlist_id FROM playlists WHERE source_url = %s",
                    (source_url,)
                )
                existing = txn.fetchone()
                if existing:
                    self.logger.info(f"Skipping duplicate playlist URL: {source_url}")
                    continue  # Skip - this playlist already scraped
            else:
                # Fallback to name+source if no URL (less reliable)
                txn.execute(
                    "SELECT playlist_id FROM playlists WHERE name = %s AND source = %s",
                    (item['name'], item.get('source', 'scraped_data'))
                )
                existing = txn.fetchone()
                if existing:
                    self.logger.info(f"Skipping duplicate playlist: {item['name']}")
                    continue

            # Insert new playlist
            txn.execute("""
                INSERT INTO playlists (name, source, source_url, playlist_type, event_date, content_hash)
                VALUES (%s, %s, %s, %s, %s, %s)
            """,
                (item['name'],
                item.get('source', 'scraped_data'),
                source_url,
                item.get('playlist_type'),
                item.get('event_date'),
                content_hash)
            )

    def _insert_playlist_tracks_batch(self, txn, batch: List[Dict[str, Any]]):
        """Insert playlist tracks batch using psycopg2 (now uses tracks table)"""
        for item in batch:
            try:
                txn.execute(
                    "SELECT playlist_id FROM playlists WHERE name = %s AND source = %s",
                    (item['playlist_name'], item.get('source', 'scraped_data'))
                )
                playlist_result = txn.fetchone()

                if not playlist_result:
                    self.logger.warning(f"Playlist not found: {item['playlist_name']}")
                    continue

                # Look up track by normalized title for better matching
                normalized_title = item['track_name'].lower().strip()
                txn.execute(
                    "SELECT id FROM tracks WHERE normalized_title = %s LIMIT 1",
                    (normalized_title,)
                )
                track_result = txn.fetchone()

                if not track_result:
                    self.logger.warning(f"Track not found: {item['track_name']}")
                    continue

                txn.execute("""
                    INSERT INTO playlist_tracks (playlist_id, position, song_id)
                    VALUES (%s, %s, %s)
                    ON CONFLICT (playlist_id, position) DO UPDATE SET
                        song_id = EXCLUDED.song_id
                """,
                    (playlist_result[0],
                    item['position'],
                    track_result[0])  # Use track_id from tracks table
                )

            except Exception as e:
                self.logger.warning(f"Could not insert playlist track: {e}")

    def _insert_track_artists_batch(self, txn, batch: List[Dict[str, Any]]):
        """Insert track-artist relationships batch (featured artists, remixers, etc.)"""
        track_artist_relationships = []

        for item in batch:
            try:
                # Look up track ID by normalized title
                normalized_title = item['track_name'].lower().strip()
                txn.execute(
                    "SELECT id FROM tracks WHERE normalized_title = %s LIMIT 1",
                    (normalized_title,)
                )
                track_result = txn.fetchone()

                # Look up artist ID by name
                txn.execute(
                    "SELECT artist_id FROM artists WHERE name = %s",
                    (item['artist_name'],)
                )
                artist_result = txn.fetchone()

                if track_result and artist_result:
                    track_artist_relationships.append((
                        track_result[0],  # track_id
                        artist_result[0],  # artist_id
                        item['role'],      # role (primary, featured, remixer, etc.)
                        item.get('position', 0)
                    ))
                else:
                    if not track_result:
                        self.logger.warning(f"Track not found for relationship: {item['track_name']}")
                    if not artist_result:
                        self.logger.warning(f"Artist not found for relationship: {item['artist_name']}")

            except Exception as e:
                self.logger.warning(f"Could not create track-artist relationship: {e}")

        # Insert all relationships
        if track_artist_relationships:
            txn.executemany("""
                INSERT INTO track_artists (track_id, artist_id, role, position)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (track_id, artist_id, role) DO NOTHING
            """, track_artist_relationships)

    def _insert_adjacency_batch(self, txn, batch: List[Dict[str, Any]]):
        """Insert track adjacency batch using psycopg2 (now uses tracks table)"""
        adjacencies_with_ids = []
        for item in batch:
            try:
                # Look up tracks by normalized title
                normalized_title1 = item['track1_name'].lower().strip()
                normalized_title2 = item['track2_name'].lower().strip()

                txn.execute(
                    "SELECT id FROM tracks WHERE normalized_title = %s LIMIT 1",
                    (normalized_title1,)
                )
                track1_result = txn.fetchone()

                txn.execute(
                    "SELECT id FROM tracks WHERE normalized_title = %s LIMIT 1",
                    (normalized_title2,)
                )
                track2_result = txn.fetchone()

                if track1_result and track2_result:
                    id1, id2 = track1_result[0], track2_result[0]
                    if str(id1) > str(id2):
                        id1, id2 = id2, id1

                    adjacencies_with_ids.append({
                        'song_id_1': id1,  # Column name unchanged in song_adjacency table
                        'song_id_2': id2,
                        'occurrence_count': item.get('occurrence_count', 1),
                        'avg_distance': item.get('distance', 1.0)
                    })
            except Exception as e:
                self.logger.warning(f"Could not create adjacency: {e}")

        if adjacencies_with_ids:
            txn.executemany("""
                INSERT INTO song_adjacency (song_id_1, song_id_2, occurrence_count, avg_distance)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (song_id_1, song_id_2) DO UPDATE SET
                    occurrence_count = song_adjacency.occurrence_count + EXCLUDED.occurrence_count,
                    avg_distance = ((song_adjacency.avg_distance * song_adjacency.occurrence_count) +
                                    (EXCLUDED.avg_distance * EXCLUDED.occurrence_count)) /
                                   (song_adjacency.occurrence_count + EXCLUDED.occurrence_count)
            """, [
                (
                    item['song_id_1'],
                    item['song_id_2'],
                    item['occurrence_count'],
                    item['avg_distance']
                ) for item in adjacencies_with_ids
            ])

    @defer.inlineCallbacks
    def flush_all_batches(self):
        """
        Flush all remaining batches SEQUENTIALLY in correct dependency order.

        Critical: Artists must be flushed before songs (songs need artist IDs).
        Songs must be flushed before playlists/adjacency (they reference songs).
        """
        # Flush in dependency order (NOT parallel - prevents NULL foreign keys)
        batch_order = ['artists', 'songs', 'playlists', 'playlist_tracks', 'track_artists', 'song_adjacency']

        for batch_type in batch_order:
            if batch_type in self.item_batches and len(self.item_batches[batch_type]) > 0:
                self.logger.info(f"üîÑ Flushing remaining {len(self.item_batches[batch_type])} {batch_type}...")
                yield self._flush_batch(batch_type)

    @defer.inlineCallbacks
    def close_spider(self, spider):
        """Flush remaining items and close connection pool using Twisted inlineCallbacks"""
        self.logger.info("üîÑ close_spider called - flushing remaining batches...")

        try:
            # Flush all remaining batches
            yield self.flush_all_batches()

            # Log validation statistics
            if PYDANTIC_AVAILABLE and self.validation_stats['total_items'] > 0:
                self.logger.info("=" * 70)
                self.logger.info("üìä DATABASE PIPELINE VALIDATION STATISTICS")
                self.logger.info("=" * 70)
                self.logger.info(f"  Total items processed: {self.validation_stats['total_items']}")
                self.logger.info(f"  ‚úÖ Valid items inserted: {self.validation_stats['valid_items']}")
                self.logger.info(f"  ‚ùå Invalid items rejected: {self.validation_stats['invalid_items']}")

                if self.validation_stats['total_items'] > 0:
                    validation_rate = (self.validation_stats['valid_items'] / self.validation_stats['total_items']) * 100
                    self.logger.info(f"  üìà Validation success rate: {validation_rate:.2f}%")

                if self.validation_stats['validation_errors']:
                    self.logger.info(f"  ‚ö†Ô∏è Sample validation errors (first 5):")
                    for i, error in enumerate(self.validation_stats['validation_errors'][:5], 1):
                        self.logger.info(f"    {i}. {error}")

                self.logger.info("=" * 70)

            # Close connection pool
            if self.dbpool:
                yield self.dbpool.close()
                self.logger.info("‚úì Database connection pool closed successfully")

            self.logger.info("‚úì Database pipeline closed successfully")

        except Exception as e:
            self.logger.error(f"‚ùå Error closing database pipeline: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
