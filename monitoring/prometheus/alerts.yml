---
# Prometheus Alert Rules for SongNodes Scraping System
# Version: 1.0
# Purpose: Detect silent failures and scraping anomalies

groups:
  # =====================================================
  # CRITICAL: Silent Failure Detection
  # =====================================================
  - name: scraping_silent_failures
    interval: 1m
    rules:
      # Alert when playlists are created with 0 tracks
      - alert: SilentScrapingFailure
        expr: |
          rate(playlists_created_total{tracklist_count="0"}[5m]) > 0
          AND
          (
            rate(playlists_created_total{tracklist_count="0"}[5m]) /
            rate(playlists_created_total[5m])
          ) > 0.1
        for: 5m
        labels:
          severity: critical
          component: scraper
          team: backend
        annotations:
          summary: "Silent scraping failure detected for {{ $labels.source }}"
          description: |
            {{ $value | humanizePercentage }} of playlists from {{ $labels.source }}
            in the last 5 minutes have 0 tracks. This indicates a parser failure.
            Check parser version {{ $labels.parsing_version }} immediately.
          dashboard: "https://grafana.songnodes.com/d/scraping-health"
          runbook: "https://docs.songnodes.com/runbooks/silent-scraping-failure"

      # Alert when NO data is extracted despite completed scrapes
      - alert: ZeroDataExtraction
        expr: |
          sum(increase(tracks_extracted_total[15m])) == 0
          AND
          sum(increase(scraping_tasks_total{status="completed"}[15m])) > 0
        for: 15m
        labels:
          severity: critical
          component: scraper
          team: backend
        annotations:
          summary: "No tracks extracted despite completed scrapes"
          description: |
            Scrapers are completing successfully but extracting 0 tracks for 15 minutes.
            This is a critical silent failure. Check parser logic immediately.
          dashboard: "https://grafana.songnodes.com/d/scraping-health"
          runbook: "https://docs.songnodes.com/runbooks/zero-data-extraction"

      # Alert on silent failures without error messages
      - alert: SilentFailureNoError
        expr: |
          rate(silent_scraping_failures_total[5m]) > 0
        for: 2m
        labels:
          severity: critical
          component: scraper
          team: backend
        annotations:
          summary: "Silent scraping failure detected without error message"
          description: |
            Playlists are being created with 0 tracks but no scrape_error field.
            Source: {{ $labels.source }}, Parser: {{ $labels.parsing_version }}
            This indicates the validation logic failed to catch the error.
          dashboard: "https://grafana.songnodes.com/d/scraping-health"
          runbook: "https://docs.songnodes.com/runbooks/silent-failure-no-error"

  # =====================================================
  # HIGH: Scraping Error Rates
  # =====================================================
  - name: scraping_errors
    interval: 1m
    rules:
      # High overall error rate
      - alert: HighScrapingErrorRate
        expr: |
          (
            sum(rate(scraping_tasks_total{status="failed"}[5m])) /
            sum(rate(scraping_tasks_total[5m]))
          ) > 0.2
        for: 5m
        labels:
          severity: warning
          component: scraper
          team: backend
        annotations:
          summary: "High scraping error rate"
          description: |
            {{ $value | humanizePercentage }} of scraping tasks are failing.
            Check scraper logs and error messages.
          dashboard: "https://grafana.songnodes.com/d/scraping-health"

      # Specific scraper failing
      - alert: ScraperFailureSpike
        expr: |
          (
            rate(scraping_tasks_total{status="failed"}[5m]) /
            rate(scraping_tasks_total[5m])
          ) by (scraper) > 0.5
        for: 5m
        labels:
          severity: warning
          component: scraper
          team: backend
        annotations:
          summary: "{{ $labels.scraper }} scraper failure spike"
          description: |
            {{ $value | humanizePercentage }} of {{ $labels.scraper }} tasks are failing.
            This scraper may need immediate attention.
          dashboard: "https://grafana.songnodes.com/d/scraping-health"

  # =====================================================
  # MEDIUM: Circuit Breaker & Health
  # =====================================================
  - name: scraping_health
    interval: 1m
    rules:
      # Circuit breaker is OPEN (service failing)
      - alert: CircuitBreakerOpen
        expr: |
          circuit_breaker_state{state="open"} == 1
        for: 5m
        labels:
          severity: warning
          component: scraper
          team: backend
        annotations:
          summary: "Circuit breaker OPEN for {{ $labels.service }}"
          description: |
            The circuit breaker for {{ $labels.service }} has been OPEN for 5 minutes.
            This means the service is failing and requests are being rejected.
            Check service health and recent errors.
          dashboard: "https://grafana.songnodes.com/d/circuit-breakers"

      # No scraping activity (system may be down)
      - alert: NoScrapingActivity
        expr: |
          rate(scraping_tasks_total[15m]) == 0
        for: 15m
        labels:
          severity: warning
          component: scraper
          team: backend
        annotations:
          summary: "No scraping activity detected"
          description: |
            No scraping tasks have been started in 15 minutes.
            Check if scraper orchestrator is running.
          dashboard: "https://grafana.songnodes.com/d/scraping-health"

  # =====================================================
  # MEDIUM: Data Quality Issues
  # =====================================================
  - name: data_quality
    interval: 5m
    rules:
      # Average tracks per playlist is suspiciously low
      - alert: LowAverageTracksPerPlaylist
        expr: |
          (
            sum(rate(tracks_extracted_total[15m])) /
            sum(rate(playlists_created_total{tracklist_count!="0"}[15m]))
          ) by (source) < 5
        for: 15m
        labels:
          severity: warning
          component: scraper
          team: backend
        annotations:
          summary: "Low average tracks per playlist for {{ $labels.source }}"
          description: |
            {{ $labels.source }} is averaging {{ $value | humanize }} tracks per playlist.
            Normal playlists should have 10-50 tracks. This may indicate partial extraction failures.
          dashboard: "https://grafana.songnodes.com/d/data-quality"

      # Parser version has high failure rate
      - alert: ParserVersionHighFailureRate
        expr: |
          (
            sum(rate(playlists_created_total{tracklist_count="0"}[1h])) by (parsing_version, source) /
            sum(rate(playlists_created_total[1h])) by (parsing_version, source)
          ) > 0.3
        for: 1h
        labels:
          severity: warning
          component: scraper
          team: backend
        annotations:
          summary: "Parser {{ $labels.parsing_version }} has high failure rate"
          description: |
            {{ $labels.parsing_version }} for {{ $labels.source }} has a {{ $value | humanizePercentage }} failure rate.
            Consider rolling back to previous version or fixing the parser.
          dashboard: "https://grafana.songnodes.com/d/parser-performance"

  # =====================================================
  # LOW: Performance & Capacity
  # =====================================================
  - name: scraping_performance
    interval: 5m
    rules:
      # Scraping queue is backing up
      - alert: ScrapingQueueBacklog
        expr: |
          scraping_queue_size{priority="high"} > 100
          OR
          scraping_queue_size{priority="critical"} > 10
        for: 10m
        labels:
          severity: info
          component: scraper
          team: backend
        annotations:
          summary: "Scraping queue backlog for {{ $labels.priority }} priority"
          description: |
            {{ $labels.priority }} priority queue has {{ $value }} items pending.
            Consider scaling up scraper workers or investigating slow tasks.
          dashboard: "https://grafana.songnodes.com/d/scraping-queue"

      # Slow scraping tasks
      - alert: SlowScrapingTasks
        expr: |
          histogram_quantile(0.95, rate(scraping_duration_seconds_bucket[10m])) > 120
        for: 15m
        labels:
          severity: info
          component: scraper
          team: backend
        annotations:
          summary: "Slow scraping tasks detected for {{ $labels.scraper }}"
          description: |
            95th percentile of {{ $labels.scraper }} task duration is {{ $value | humanizeDuration }}.
            Tasks should complete in under 2 minutes. Check for network issues or rate limiting.
          dashboard: "https://grafana.songnodes.com/d/scraping-performance"

  # =====================================================
  # Database-Level Validation Failures
  # =====================================================
  - name: database_validation
    interval: 5m
    rules:
      # Database constraint violations
      - alert: DatabaseConstraintViolations
        expr: |
          rate(database_operations_total{operation="insert_playlist",status="constraint_violation"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
          component: database
          team: backend
        annotations:
          summary: "Database constraint violations detected"
          description: |
            Playlists are being rejected by database constraints (likely 0 tracks with no error).
            This indicates the validation pipeline is catching issues correctly.
            Review logs to ensure errors are being properly logged upstream.
          dashboard: "https://grafana.songnodes.com/d/database-health"

# =====================================================
# Alert Routing Configuration
# =====================================================
# Add to alertmanager.yml:
#
# route:
#   group_by: ['alertname', 'source', 'scraper']
#   group_wait: 10s
#   group_interval: 5m
#   repeat_interval: 12h
#   receiver: 'engineering-alerts'
#   routes:
#     - match:
#         severity: critical
#       receiver: 'pagerduty-critical'
#       group_wait: 0s
#       repeat_interval: 5m
#     - match:
#         severity: warning
#       receiver: 'slack-engineering'
#       group_interval: 10m
#       repeat_interval: 4h
#
# receivers:
#   - name: 'pagerduty-critical'
#     pagerduty_configs:
#       - service_key: '<pagerduty-key>'
#   - name: 'slack-engineering'
#     slack_configs:
#       - api_url: '<slack-webhook>'
#         channel: '#engineering-alerts'
#   - name: 'engineering-alerts'
#     slack_configs:
#       - api_url: '<slack-webhook>'
#         channel: '#engineering-alerts'
