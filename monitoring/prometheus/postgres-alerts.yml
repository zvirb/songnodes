# PostgreSQL and Connection Pool Alerting Rules
# Target: 20,000+ tracks/hour, <50ms query time, <100ms API response

groups:
  - name: postgres.rules
    rules:
      # Database connection monitoring
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: PostgreSQL server is down
          description: "PostgreSQL server {{ $labels.instance }} is down"

      - alert: PostgreSQLHighConnections
        expr: pg_stat_database_numbackends / pg_settings_max_connections * 100 > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: PostgreSQL connection usage is high
          description: "PostgreSQL connection usage is {{ $value }}% on {{ $labels.instance }}"

      # Query performance monitoring
      - alert: PostgreSQLSlowQueries
        expr: rate(pg_stat_statements_mean_time_ms[5m]) > 100
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: PostgreSQL queries are running slowly
          description: "Average query time is {{ $value }}ms on {{ $labels.instance }}"

      - alert: PostgreSQLVerySlowQueries
        expr: rate(pg_stat_statements_mean_time_ms[5m]) > 500
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: PostgreSQL queries are very slow
          description: "Average query time is {{ $value }}ms on {{ $labels.instance }}"

      # Lock monitoring
      - alert: PostgreSQLDeadlocks
        expr: rate(pg_stat_database_deadlocks[5m]) > 0.1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: PostgreSQL deadlocks detected
          description: "{{ $value }} deadlocks per second on {{ $labels.instance }}"

      # Disk space monitoring
      - alert: PostgreSQLDiskSpaceLow
        expr: pg_database_size_bytes / (1024*1024*1024) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: PostgreSQL database size is growing
          description: "Database size is {{ $value }}GB on {{ $labels.instance }}"

      # WAL monitoring
      - alert: PostgreSQLWALFilesHigh
        expr: pg_stat_archiver_archived_count - pg_stat_archiver_archived_count offset 1h > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High WAL file generation
          description: "{{ $value }} WAL files generated in the last hour on {{ $labels.instance }}"

  - name: pgbouncer.rules
    rules:
      # Connection pool monitoring
      - alert: PgBouncerDown
        expr: pgbouncer_active_connections_total == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: PgBouncer connection pool is down
          description: "PgBouncer pool {{ $labels.database }} has no active connections"

      - alert: PgBouncerHighWaitingConnections
        expr: pgbouncer_waiting_connections_total > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: High number of waiting connections in PgBouncer
          description: "{{ $value }} connections waiting in pool {{ $labels.database }}"

      - alert: PgBouncerPoolUtilizationHigh
        expr: (pgbouncer_used_connections_total / (pgbouncer_used_connections_total + pgbouncer_waiting_connections_total)) * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: PgBouncer pool utilization is high
          description: "Pool utilization is {{ $value }}% for database {{ $labels.database }}"

      - alert: PgBouncerConnectionErrors
        expr: rate(pgbouncer_connection_errors_total[5m]) > 0.1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: PgBouncer connection errors
          description: "{{ $value }} connection errors per second in pool {{ $labels.database }}"

  - name: musicdb_performance.rules
    rules:
      # Music database specific alerts
      - alert: HighTrackProcessingLatency
        expr: histogram_quantile(0.95, rate(pgbouncer_query_duration_seconds_bucket[5m])) > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Track processing latency is high
          description: "95th percentile query time is {{ $value }}s (target: <0.05s)"

      - alert: LowTrackProcessingThroughput
        expr: rate(tracks_processed_total[5m]) < 5.5  # 20,000/hour = 5.56/second
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Track processing throughput is below target
          description: "Processing {{ $value }} tracks/second (target: >5.56/second for 20,000/hour)"

      - alert: DataQualityIssuesHigh
        expr: rate(data_quality_issues_created_total[1h]) > 100
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: High rate of data quality issues
          description: "{{ $value }} data quality issues created in the last hour"

      # Index usage monitoring
      - alert: UnusedIndexes
        expr: pg_stat_user_indexes_idx_scan == 0
        for: 24h
        labels:
          severity: info
        annotations:
          summary: Unused database index detected
          description: "Index {{ $labels.indexname }} on table {{ $labels.tablename }} has not been used"

      # Materialized view freshness
      - alert: MaterializedViewStale
        expr: time() - pg_stat_user_tables_last_autoanalyze > 3600
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Materialized view may be stale
          description: "Table {{ $labels.tablename }} was last analyzed {{ $value }}s ago"

      # Bulk processing monitoring
      - alert: BulkInsertFailureRate
        expr: rate(bulk_insert_errors_total[5m]) / rate(bulk_insert_attempts_total[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: High bulk insert failure rate
          description: "Bulk insert failure rate is {{ $value }}% (target: <5%)"

  - name: system_resources.rules
    rules:
      # System resource monitoring for database performance
      - alert: HighCPUUsage
        expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High CPU usage
          description: "CPU usage is {{ $value }}% on {{ $labels.instance }}"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High memory usage
          description: "Memory usage is {{ $value }}% on {{ $labels.instance }}"

      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High disk usage
          description: "Disk usage is {{ $value }}% on {{ $labels.instance }}"

      - alert: HighIOWait
        expr: rate(node_cpu_seconds_total{mode="iowait"}[5m]) * 100 > 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High I/O wait time
          description: "I/O wait time is {{ $value }}% on {{ $labels.instance }}"