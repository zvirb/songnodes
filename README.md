# ğŸµ SongNodes - Music Data Visualization & Analysis Platform

## ğŸµ Overview

SongNodes is a comprehensive music data platform that combines intelligent scraping, graph visualization, and real-time analytics. Built with containerized microservices and AI-powered data processing, it provides interactive graph-based insights into music relationships, track adjacencies, and performance analytics.

## ğŸš€ Key Features

- **Interactive Graph Visualization**: D3.js force-directed graphs with WebGL acceleration showing track relationships and adjacencies
- **Target Tracks Management**: User-driven feedback loop for managing tracks to scrape and analyze
- **AI-Powered Scraping**: GPU-accelerated Ollama integration for adaptive HTML analysis and selector recovery
- **Multi-Source Data Collection**: Automated scraping from 1001tracklists, MixesDB, Setlist.fm, Reddit, and more
- **Real-time Analytics**: RESTful and GraphQL APIs with WebSocket support for live updates
- **Comprehensive Monitoring**: Prometheus, Grafana, and ELK stack integration
- **Containerized Architecture**: Full Docker Compose deployment with service isolation and health checks

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Nginx Reverse Proxy                      â”‚
â”‚                    (Ports: 8443, 8088)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API Gateway                             â”‚
â”‚                      (Port: 8080)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ REST API  â”‚  â”‚GraphQL  â”‚  â”‚WebSocket  â”‚
    â”‚Port: 8082 â”‚  â”‚Port:8081â”‚  â”‚Port: 8083â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Data Processing Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚Transformer   â”‚ â”‚NLP Processor â”‚ â”‚Validator     â”‚        â”‚
â”‚ â”‚Port: 8020    â”‚ â”‚Port: 8021    â”‚ â”‚Port: 8022    â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Scraping Services                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚1001tracks â”‚ â”‚MixesDB     â”‚ â”‚Setlist.fm  â”‚ â”‚Reddit    â”‚â”‚
â”‚ â”‚Port: 8011  â”‚ â”‚Port: 8012  â”‚ â”‚Port: 8013  â”‚ â”‚Port:8014 â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Storage                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚ â”‚PostgreSQL    â”‚ â”‚Redis Cache   â”‚ â”‚RabbitMQ      â”‚        â”‚
â”‚ â”‚Port: 5433    â”‚ â”‚Port: 6380    â”‚ â”‚Port: 5673    â”‚        â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technology Stack

- **Frontend**: React 18 + TypeScript + Vite + Redux Toolkit + D3.js
- **Backend**: Python FastAPI microservices with async/await
- **AI/LLM**: Ollama with GPU acceleration (RTX 4050) + Llama 3.2 3B
- **Database**: PostgreSQL 15 with JSONB support and connection pooling
- **Cache/Queue**: Redis 7 + RabbitMQ for message queuing
- **APIs**: RESTful APIs, GraphQL, WebSocket real-time connections
- **Monitoring**: Prometheus + Grafana + Elasticsearch + Kibana
- **Containerization**: Docker + Docker Compose with health checks
- **Visualization**: D3.js force simulation with WebGL acceleration

## ğŸ“ Project Structure

```
songnodes/
â”œâ”€â”€ musicdb_scrapy/           # Cloned scraper repository
â”œâ”€â”€ docs/                     # Comprehensive documentation
â”‚   â”œâ”€â”€ sdlc/                # SDLC documentation
â”‚   â”œâ”€â”€ architecture/        # System architecture
â”‚   â”œâ”€â”€ orchestration/       # Workflow implementation
â”‚   â””â”€â”€ deployment/          # Deployment guides
â”œâ”€â”€ services/                # Microservice implementations
â”‚   â”œâ”€â”€ scraper-orchestrator/
â”‚   â”œâ”€â”€ data-transformer/
â”‚   â”œâ”€â”€ api-gateway/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sql/                     # Database schemas
â”‚   â””â”€â”€ init/
â”œâ”€â”€ monitoring/              # Monitoring configurations
â”‚   â”œâ”€â”€ prometheus/
â”‚   â””â”€â”€ grafana/
â”œâ”€â”€ .claude/                 # Orchestration configs
â”‚   â”œâ”€â”€ unified-orchestration-config.yaml
â”‚   â””â”€â”€ orchestration_todos.json
â”œâ”€â”€ docker-compose.yml       # Main Docker configuration
â””â”€â”€ README.md               # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Docker 20.10+
- Docker Compose 2.0+
- 16GB RAM minimum
- 100GB disk space

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/zvirb/musicdb_scrapy.git
cd songnodes
```

2. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your API keys and passwords
```

3. **Start the services**
```bash
# Start essential services (MANDATORY: Use Docker Compose)
docker compose up -d postgres redis rest-api api-gateway

# Start frontend UI
docker compose up -d frontend

# Start scraping services (optional)
docker compose up -d scraper-orchestrator scraper-1001tracklists

# Start monitoring (optional)
docker compose up -d prometheus grafana
```

4. **Verify services are running**
```bash
docker compose ps
curl http://localhost:8080/health
curl http://localhost:8082/health
```

5. **Access the services**
- **SongNodes UI**: http://localhost:3006 (Main interface)
- **API Gateway**: http://localhost:8080 (REST endpoints)
- **Target Tracks API**: http://localhost:8082/api/v1/target-tracks
- **GraphQL Playground**: http://localhost:8081/graphql
- **Grafana Dashboard**: http://localhost:3001 (admin/admin)
- **Ollama AI**: http://localhost:11434

## âœ… Current System Status (Sept 2025)

### Working Features
- âœ… **REST API Connectivity**: Fixed port exposure, now accessible from frontend
- âœ… **API Gateway Routing**: Complete proxy routing for target tracks endpoints
- âœ… **Target Tracks CRUD**: Full create/read/update/delete operations working
- âœ… **User Feedback Loop**: Frontend â†’ API Gateway â†’ REST API pathway verified
- âœ… **TypeScript Compilation**: Fixed type definitions and imports
- âœ… **Docker System**: Optimized with 60GB+ cleanup, services running healthy

### API Endpoints Working
```bash
# List target tracks
curl http://localhost:8080/api/v1/target-tracks

# Create new target track
curl -X POST http://localhost:8080/api/v1/target-tracks \
  -H "Content-Type: application/json" \
  -d '{"title": "Test Track", "artist": "Test Artist", "priority": "medium"}'

# Trigger search for track
curl -X POST http://localhost:8080/api/v1/target-tracks/1/search
```

### Service Health
- ğŸŸ¢ PostgreSQL (port 5433) - Healthy
- ğŸŸ¢ Redis (port 6380) - Healthy
- ğŸŸ¢ REST API (port 8082) - Healthy
- ğŸŸ¢ API Gateway (port 8080) - Healthy
- ğŸŸ¢ Frontend (port 3006) - Building and running

## ğŸ“‹ Orchestration Workflow

The project follows a 12-step orchestration workflow:

1. **Todo Context Integration** - Load and prioritize tasks
2. **Agent Ecosystem Validation** - Verify all services are operational
3. **Strategic Intelligence Planning** - Analyze patterns and plan execution
4. **Multi-Domain Research Discovery** - Parallel research across domains
5. **Context Synthesis & Compression** - Integrate findings efficiently
6. **Parallel Implementation Execution** - Multi-stream task execution
7. **Evidence-Based Validation** - Validate with concrete evidence
8. **Decision & Iteration Control** - Analyze and iterate if needed
9. **Atomic Version Control** - Commit changes atomically
10. **Meta-Orchestration Audit** - Analyze workflow effectiveness
11. **Production Deployment** - Blue-green deployment
12. **Production Validation** - Monitor and loop control

## ğŸ”Œ API Usage

### REST API Example
```bash
# Get tracks
curl http://localhost:8082/api/v1/tracks?limit=10

# Search artists
curl http://localhost:8082/api/v1/artists/search?q=deadmau5
```

### GraphQL Example
```graphql
query {
  tracks(limit: 10, genre: "Tech House") {
    id
    title
    artists {
      name
      role
    }
    playCount
  }
}
```

### WebSocket Connection
```javascript
const ws = new WebSocket('ws://localhost:8083/live');
ws.onmessage = (event) => {
  console.log('New track scraped:', JSON.parse(event.data));
};
```

## ğŸ“Š Monitoring

Access monitoring dashboards:
- **Grafana**: http://localhost:3001
  - Scraping metrics
  - API performance
  - System resources
- **Prometheus**: http://localhost:9091
- **Kibana**: http://localhost:5602

## ğŸ§ª Testing

```bash
# Run unit tests
docker-compose exec scraper-orchestrator pytest tests/unit/

# Run integration tests
docker-compose exec scraper-orchestrator pytest tests/integration/

# Run end-to-end tests
./scripts/run-e2e-tests.sh
```

## ğŸš¢ Deployment

### Production Deployment
```bash
# Use production compose file
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Run database migrations
docker-compose exec postgres psql -U musicdb_user -d musicdb -f /migrations/latest.sql

# Verify deployment
./scripts/verify-deployment.sh
```

### Scaling
```bash
# Scale scrapers
docker-compose up -d --scale scraper-1001tracklists=5

# Scale API services
docker-compose up -d --scale rest-api=3
```

## ğŸ“ Documentation

Comprehensive documentation available in `/docs`:
- [SDLC Overview](docs/sdlc/01-project-overview.md)
- [Container Architecture](docs/architecture/container-architecture.md)
- [Orchestration Workflow](docs/orchestration/workflow-implementation.md)
- [Deployment Guide](docs/deployment/deployment-guide.md)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built using the 12-step agentic orchestration workflow
- Scrapy framework for web scraping
- PostgreSQL for robust data storage
- Docker for containerization

## ğŸ“ Support

For issues or questions:
- Create an issue on GitHub
- Check logs: `docker-compose logs [service_name]`
- Review orchestration status: http://localhost:8001/orchestration/status

---

**Note**: This project uses non-standard ports to avoid conflicts. See the [port reference](docs/deployment/deployment-guide.md#port-reference) for details.