name: MusicDB CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM for scraper runs

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ===========================================
  # TESTING PHASE
  # ===========================================
  
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache Dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-asyncio black flake8 mypy
    
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Format Check with Black
      run: black --check .
    
    - name: Type Check with MyPy
      run: mypy musicdb_scrapy/
    
    - name: Run Unit Tests
      run: |
        pytest tests/unit/ -v --cov=musicdb_scrapy --cov-report=xml
    
    - name: Upload Coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # ===========================================
  # SECURITY SCANNING
  # ===========================================
  
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy results to GitHub Security
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Run Bandit Security Scan
      run: |
        pip install bandit
        bandit -r musicdb_scrapy/ -f json -o bandit-report.json
    
    - name: Upload Bandit results
      uses: actions/upload-artifact@v3
      with:
        name: bandit-report
        path: bandit-report.json

  # ===========================================
  # BUILD PHASE
  # ===========================================
  
  build:
    name: Build Docker Images
    runs-on: ubuntu-latest
    needs: [test, security]
    if: github.event_name == 'push'
    
    strategy:
      matrix:
        service:
          - scraper-1001tracklists
          - scraper-mixesdb
          - scraper-setlistfm
          - data-transformer
          - api-gateway
          - rest-api
          - graphql-api
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Log in to GitHub Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha,prefix={{branch}}-
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        file: ./services/${{ matrix.service }}/Dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  # ===========================================
  # INTEGRATION TESTING
  # ===========================================
  
  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: build
    if: github.event_name == 'push'
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: musicdb_test
        ports:
          - 5433:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7-alpine
        ports:
          - 6380:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install Dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-asyncio
    
    - name: Initialize Database
      run: |
        PGPASSWORD=test_password psql -h localhost -p 5433 -U postgres -d musicdb_test -f sql/init/01-schema.sql
    
    - name: Run Integration Tests
      env:
        DATABASE_URL: postgresql://postgres:test_password@localhost:5433/musicdb_test
        REDIS_HOST: localhost
        REDIS_PORT: 6380
      run: |
        pytest tests/integration/ -v

  # ===========================================
  # DEPLOY TO STAGING
  # ===========================================
  
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: integration-test
    if: github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://staging.musicdb.example.com
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Staging Server
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.STAGING_HOST }}
        username: ${{ secrets.STAGING_USER }}
        key: ${{ secrets.STAGING_SSH_KEY }}
        script: |
          cd /opt/musicdb
          git pull origin develop
          docker-compose -f docker-compose.yml -f docker-compose.staging.yml pull
          docker-compose -f docker-compose.yml -f docker-compose.staging.yml up -d --remove-orphans
          docker-compose exec -T postgres pg_isready -U musicdb_user
          sleep 10
          ./scripts/run-health-checks.sh
    
    - name: Run Smoke Tests
      run: |
        curl -f https://staging.musicdb.example.com/api/v1/health || exit 1
        curl -f https://staging.musicdb.example.com/api/v1/scrapers/status || exit 1

  # ===========================================
  # DEPLOY TO PRODUCTION
  # ===========================================
  
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: integration-test
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://musicdb.example.com
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Create Database Backup
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.PROD_HOST }}
        username: ${{ secrets.PROD_USER }}
        key: ${{ secrets.PROD_SSH_KEY }}
        script: |
          cd /opt/musicdb
          ./scripts/backup-database.sh
    
    - name: Blue-Green Deployment
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.PROD_HOST }}
        username: ${{ secrets.PROD_USER }}
        key: ${{ secrets.PROD_SSH_KEY }}
        script: |
          cd /opt/musicdb
          
          # Pull latest images
          git pull origin main
          docker-compose -f docker-compose.yml -f docker-compose.prod.yml pull
          
          # Deploy to green environment
          docker-compose -f docker-compose.yml -f docker-compose.prod.yml \
            -p musicdb-green up -d --remove-orphans
          
          # Wait for services to be healthy
          sleep 30
          ./scripts/run-health-checks.sh green
          
          # Switch traffic to green
          ./scripts/switch-traffic.sh green
          
          # Stop blue environment after successful switch
          sleep 60
          docker-compose -f docker-compose.yml -f docker-compose.prod.yml \
            -p musicdb-blue down
    
    - name: Verify Production Deployment
      run: |
        curl -f https://musicdb.example.com/api/v1/health || exit 1
        curl -f https://musicdb.example.com/api/v1/scrapers/status || exit 1
    
    - name: Send Deployment Notification
      if: always()
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        text: 'Production deployment ${{ job.status }}'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  # ===========================================
  # SCHEDULED SCRAPING JOBS
  # ===========================================
  
  scheduled-scraping:
    name: Scheduled Scraping
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 2 * * *'
    
    steps:
    - name: Trigger Scraping Jobs
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.PROD_HOST }}
        username: ${{ secrets.PROD_USER }}
        key: ${{ secrets.PROD_SSH_KEY }}
        script: |
          cd /opt/musicdb
          
          # Start orchestration workflow
          docker-compose exec -T scraper-orchestrator \
            python -m orchestration.start \
            --config /app/.claude/unified-orchestration-config.yaml \
            --step 0 \
            --mode scheduled
          
          # Monitor for 30 minutes
          timeout 1800 docker-compose logs -f scraper-orchestrator | \
            grep -E "(SUCCESS|ERROR|COMPLETE)" || true
    
    - name: Generate Scraping Report
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.PROD_HOST }}
        username: ${{ secrets.PROD_USER }}
        key: ${{ secrets.PROD_SSH_KEY }}
        script: |
          cd /opt/musicdb
          docker-compose exec -T scraper-orchestrator \
            python -m reporting.generate \
            --type daily \
            --output /reports/daily-$(date +%Y%m%d).json
    
    - name: Send Daily Report
      uses: 8398a7/action-slack@v3
      with:
        status: custom
        custom_payload: |
          {
            text: 'Daily Scraping Report',
            attachments: [{
              color: 'good',
              title: 'MusicDB Daily Scraping Complete',
              fields: [
                { title: 'Date', value: '${{ env.DATE }}', short: true },
                { title: 'Status', value: 'Complete', short: true }
              ]
            }]
          }
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  # ===========================================
  # MONITORING & ALERTING
  # ===========================================
  
  monitor-health:
    name: Health Monitoring
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Check Service Health
      run: |
        # Check all service endpoints
        for endpoint in \
          "https://musicdb.example.com/api/v1/health" \
          "https://musicdb.example.com/api/v1/scrapers/status" \
          "https://musicdb.example.com/metrics"; do
          
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" $endpoint)
          if [ $STATUS -ne 200 ]; then
            echo "Health check failed for $endpoint (status: $STATUS)"
            exit 1
          fi
        done
    
    - name: Check Data Quality
      uses: appleboy/ssh-action@v0.1.5
      with:
        host: ${{ secrets.PROD_HOST }}
        username: ${{ secrets.PROD_USER }}
        key: ${{ secrets.PROD_SSH_KEY }}
        script: |
          cd /opt/musicdb
          docker-compose exec -T postgres psql -U musicdb_user -d musicdb -c \
            "SELECT COUNT(*) FROM data_quality_issues WHERE is_resolved = FALSE AND severity = 'critical';"