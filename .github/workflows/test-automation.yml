name: Test Automation CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  POSTGRES_PASSWORD: test_password
  REDIS_PASSWORD: test_password

jobs:
  # Pre-flight checks
  pre-flight:
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.check.outputs.should-run }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 2
      
      - name: Check if tests should run
        id: check
        run: |
          if git diff --name-only HEAD~1 | grep -E '\.(py|js|ts|yml|yaml|json)$' || [ "${{ github.event_name }}" = "schedule" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
          fi

  # Code quality checks
  code-quality:
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements.txt
      
      - name: Run code formatting check
        run: |
          black --check services/ musicdb_scrapy/ tests/
          isort --check-only services/ musicdb_scrapy/ tests/
      
      - name: Run linting
        run: |
          flake8 services/ musicdb_scrapy/ tests/ --max-line-length=100
      
      - name: Run type checking
        run: |
          mypy services/ --ignore-missing-imports
      
      - name: Security scan
        run: |
          bandit -r services/ musicdb_scrapy/ -f json -o security-report.json || true
          safety check --json --output safety-report.json || true
      
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            security-report.json
            safety-report.json

  # Unit tests
  unit-tests:
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      
      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('tests/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements.txt
      
      - name: Run unit tests
        run: |
          pytest tests/unit/ \
            --tb=short \
            --cov=services \
            --cov=musicdb_scrapy \
            --cov-report=xml \
            --cov-report=term-missing \
            --junitxml=unit-test-results.xml \
            --maxfail=10
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results-${{ matrix.python-version }}
          path: |
            unit-test-results.xml
            coverage.xml
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.python-version == '3.11'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  # Integration tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight, unit-tests]
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: test_musicdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
      
      rabbitmq:
        image: rabbitmq:3.12-management
        env:
          RABBITMQ_DEFAULT_USER: test
          RABBITMQ_DEFAULT_PASS: test
        options: >-
          --health-cmd "rabbitmq-diagnostics -q ping"
          --health-interval 30s
          --health-timeout 30s
          --health-retries 3
        ports:
          - 5672:5672
          - 15672:15672
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements.txt
      
      - name: Set up test environment
        env:
          DATABASE_URL: postgresql://postgres:${{ env.POSTGRES_PASSWORD }}@localhost:5432/test_musicdb
          REDIS_HOST: localhost
          REDIS_PORT: 6379
          RABBITMQ_HOST: localhost
          RABBITMQ_PORT: 5672
        run: |
          # Wait for services to be ready
          sleep 10
          
          # Run database migrations if needed
          # alembic upgrade head || true
      
      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:${{ env.POSTGRES_PASSWORD }}@localhost:5432/test_musicdb
          REDIS_HOST: localhost
          RABBITMQ_HOST: localhost
        run: |
          pytest tests/integration/ \
            --tb=short \
            --junitxml=integration-test-results.xml \
            --maxfail=5
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: integration-test-results.xml

  # Performance tests
  performance-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight, integration-tests]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && github.event_name != 'pull_request'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements.txt
      
      - name: Start test services
        run: |
          # Start minimal services for performance testing
          docker-compose -f docker-compose.test.yml up -d postgres redis
          sleep 15
      
      - name: Run performance tests
        run: |
          # Run lightweight performance tests suitable for CI
          pytest tests/performance/ \
            -m "not slow" \
            --tb=short \
            --junitxml=performance-test-results.xml \
            --timeout=300
      
      - name: Run load tests
        run: |
          # Run Locust tests with limited duration for CI
          locust -f tests/performance/load_tests.py \
            --host=http://localhost:8001 \
            --users=10 \
            --spawn-rate=2 \
            --run-time=60s \
            --html=performance-report.html \
            --headless
      
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: |
            performance-test-results.xml
            performance-report.html
      
      - name: Stop test services
        if: always()
        run: |
          docker-compose -f docker-compose.test.yml down

  # End-to-end tests
  e2e-tests:
    runs-on: ubuntu-latest
    needs: [pre-flight, integration-tests]
    if: needs.pre-flight.outputs.should-run-tests == 'true' && github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements.txt
          npm install
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium
      
      - name: Start full application stack
        run: |
          docker-compose up -d
          sleep 30  # Wait for all services to start
      
      - name: Run E2E tests
        run: |
          pytest tests/e2e/ \
            --tb=short \
            --junitxml=e2e-test-results.xml \
            --timeout=600
      
      - name: Upload E2E test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-test-results
          path: |
            e2e-test-results.xml
            tests/e2e/screenshots/
      
      - name: Stop application stack
        if: always()
        run: |
          docker-compose down

  # Test reporting
  test-report:
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, performance-tests]
    if: always() && needs.pre-flight.outputs.should-run-tests == 'true'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements.txt
      
      - name: Download all artifacts
        uses: actions/download-artifact@v3
        with:
          path: tests/reports/
      
      - name: Generate comprehensive test report
        run: |
          python tests/utils/generate_test_report.py
      
      - name: Upload test report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-test-report
          path: tests/reports/
      
      - name: Comment PR with test results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = 'tests/reports/executive-summary.md';
            
            if (fs.existsSync(path)) {
              const summary = fs.readFileSync(path, 'utf8');
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## Test Results Summary\n\n${summary}`
              });
            }

  # Nightly comprehensive tests
  nightly-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/requirements.txt
      
      - name: Run comprehensive test suite
        run: |
          make test-all
      
      - name: Upload nightly test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: nightly-test-results
          path: tests/reports/
      
      - name: Send Slack notification on failure
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          text: 'Nightly tests failed for SongNodes project'
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  # Quality gates
  quality-gate:
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests]
    if: always() && needs.pre-flight.outputs.should-run-tests == 'true'
    
    steps:
      - name: Check quality gate
        run: |
          echo "Checking quality gates..."
          
          # Check if critical jobs passed
          if [[ "${{ needs.code-quality.result }}" != "success" ]]; then
            echo "❌ Code quality checks failed"
            exit 1
          fi
          
          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
            echo "❌ Unit tests failed"
            exit 1
          fi
          
          if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
            echo "❌ Integration tests failed"
            exit 1
          fi
          
          echo "✅ All quality gates passed"
      
      - name: Set quality gate status
        if: github.event_name == 'pull_request'
        run: |
          echo "Quality gate status: PASSED"

# Workflow completion notification
  notify-completion:
    runs-on: ubuntu-latest
    needs: [quality-gate, test-report]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
      - name: Notify completion
        run: |
          echo "Test automation workflow completed"
          echo "Quality gate: ${{ needs.quality-gate.result }}"
          echo "Test report: ${{ needs.test-report.result }}"